{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Experiments - Multi-label CNNText Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayn/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# matplotlib.rcParams['figure.figsize'] = [5, 10]\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, hamming_loss\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from fastai import text as ft\n",
    "from fastai import dataloader as fd\n",
    "from fastai import dataset as fs\n",
    "from fastai import learner as fl\n",
    "from fastai import core as fc\n",
    "from fastai import metrics as fm\n",
    "\n",
    "\n",
    "from skai.runner import TextRunner, Adam_lambda\n",
    "from skai.mwrapper import MWrapper, SKModel\n",
    "from skai.utils import multi_to_text_out, vote_pred\n",
    "from skai.utils import get_classification_type, weights_init, multilabel_prediction, prf_report\n",
    "from skai.dataset import TokenDataset, SimpleDataset\n",
    "from skai.metrics import f1_micro_skai\n",
    "\n",
    "\n",
    "def mapt(f, *iters):\n",
    "    return tuple(map(f, *iters))\n",
    "\n",
    "def mapl(f, *iters):\n",
    "    return list(map(f, *iters))\n",
    "\n",
    "def manually_remove_problems(data):\n",
    "    \"\"\" remove problem from data if it has a certain tag\"\"\"\n",
    "    final_data = {}\n",
    "    remove = ['*special']\n",
    "    for i in data:\n",
    "        if set(data[i][1][0]).intersection(set(remove)) == set():\n",
    "            if data[i][0][0] != '':\n",
    "                final_data[i] = data[i]\n",
    "    return final_data\n",
    "\n",
    "def get_single_label_problems(data):\n",
    "    '''returns a dict of all problems which only have one label'''\n",
    "    single_label_problems = {}\n",
    "    for i in data:\n",
    "        if len(data[i][1][0]) == 1:\n",
    "            single_label_problems[i] = data[i]\n",
    "    return single_label_problems\n",
    "\n",
    "def get_classwise_distribution(data):\n",
    "    class_count = {}\n",
    "    for i in data:\n",
    "        for cls in data[i][1][0]:\n",
    "            if cls in class_count:\n",
    "                class_count[cls] +=1 \n",
    "            else:\n",
    "                class_count[cls] = 1\n",
    "    return class_count\n",
    "\n",
    "\n",
    "def get_topk_single_label_problems(data,k):\n",
    "    \"\"\" get top k by frequency single label problems\"\"\"\n",
    "    class_dict = get_classwise_distribution(data)\n",
    "    print(class_dict)\n",
    "    class_dict = dict(sorted(class_dict.items(), key=itemgetter(1), reverse=True)[:k])\n",
    "    print(set(class_dict.keys()))\n",
    "\n",
    "    topk_data = {}\n",
    "    for i in data:\n",
    "        if set(data[i][1][0]).intersection(set(class_dict.keys())) != set():\n",
    "            topk_data[i] = data[i]\n",
    "            \n",
    "    return topk_data\n",
    "\n",
    "def make_text_dataset(rdata):\n",
    "    Xtext, ytext = [], []\n",
    "    for url, data in rdata.items():\n",
    "        try:\n",
    "            ytext.append(data[1][0][0])\n",
    "        except IndexError:\n",
    "            continue\n",
    "        Xtext.append(data[0][0])\n",
    "    return Xtext, ytext\n",
    "\n",
    "def make_multi_text_dataset(rdata):\n",
    "    Xtext, ytext = [], []\n",
    "    for url, data in rdata.items():\n",
    "        try:\n",
    "            ytext.append(data[1][0])\n",
    "        except IndexError:\n",
    "            continue\n",
    "        Xtext.append(data[0][0])\n",
    "    return Xtext, ytext\n",
    "\n",
    "def make_statement_dataset(rdata):\n",
    "    Xtext, ytext = [], []\n",
    "    for url, data in rdata.items():\n",
    "        try:\n",
    "            ytext.append(data[1][0][0])\n",
    "        except IndexError:\n",
    "            continue\n",
    "        Xtext.append(data[0][2])\n",
    "    return Xtext, ytext\n",
    "\n",
    "def make_non_statement_dataset(rdata):\n",
    "    Xtext, ytext = [], []\n",
    "    for url, data in rdata.items():\n",
    "        try:\n",
    "            ytext.append(data[1][0][0])\n",
    "        except IndexError:\n",
    "            continue\n",
    "        Xtext.append(f'{data[0][3]}\\n{data[0][4]}\\n{data[0][5]}')\n",
    "    return Xtext, ytext\n",
    "\n",
    "def make_multi_statement_dataset(rdata):\n",
    "    Xtext, ytext = [], []\n",
    "    for url, data in rdata.items():\n",
    "        try:\n",
    "            ytext.append(data[1][0])\n",
    "        except IndexError:\n",
    "            continue\n",
    "        Xtext.append(data[0][2])\n",
    "    return Xtext, ytext\n",
    "\n",
    "def make_multi_io_dataset(rdata):\n",
    "    Xtext, ytext = [], []\n",
    "    for url, data in rdata.items():\n",
    "        try:\n",
    "            ytext.append(data[1][0])\n",
    "        except IndexError:\n",
    "            continue\n",
    "        Xtext.append(f'{data[0][3]}\\n{data[0][4]}\\n{data[0][5]}')\n",
    "    return Xtext, ytext\n",
    "\n",
    "def get_class_list(labels):\n",
    "    return list(set(labels))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(22, 16)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=1.0)\n",
    "#     plt.title(title, fontsize)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=32)\n",
    "    plt.yticks(tick_marks, classes, fontsize=32)\n",
    "\n",
    "    print(cm.max())\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = 0.5\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                 fontsize=32)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=32)\n",
    "    plt.xlabel('Predicted label', fontsize=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top10m = pickle.load(open('data/10multi_26aug.pkl', 'rb'))\n",
    "top20m = pickle.load(open('data/20multi_26aug.pkl', 'rb'))\n",
    "\n",
    "top10pm, top20pm = mapt(make_multi_statement_dataset, [top10m, top20m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3737\n"
     ]
    }
   ],
   "source": [
    "print(len(top10pm[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary search', 'implementation', 'data structures']\n",
      "['binary search', 'data structures', 'brute force', 'dp']\n"
     ]
    }
   ],
   "source": [
    "print(top20pm[1][0])\n",
    "print(top10pm[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    def __init__(self, embed_num, class_num, channel_in=1, \n",
    "                 kernel_sizes=[3, 4, 5], kernel_num=512, embed_dim=300):\n",
    "        super().__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.embed = nn.Embedding(embed_num, embed_dim)\n",
    "        \n",
    "        convs = [nn.Conv1d(1, kernel_num, (ks, embed_dim))\n",
    "                 for ks in kernel_sizes]\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "#         self.bn1 = nn.BatchNorm2d(kernel_num)\n",
    "        self.fc1 = nn.Linear(len(kernel_sizes) * kernel_num, class_num)\n",
    "    \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x, 1)\n",
    "        \n",
    "        out = self.fc1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20-multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint reached: raw data cleaned.\n",
      "multilabel classification.\n"
     ]
    }
   ],
   "source": [
    "trunner = TextRunner([None], top20pm[0], top20pm[1], 'top20pm')\n",
    "in_dim = len(trunner.alldata.tvectorizer.itos)\n",
    "Xall, yall = trunner.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03488dcc91245d0ae69520c63e02da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayn/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aayn/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_micro_skai \n",
      "    0      0.085779   0.081629   0.06711   \n",
      " 62%|██████▎   | 70/112 [00:02<00:01, 23.34it/s, loss=0.0813]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2ba43522c7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                                              opt_fn=Adam_lambda())\n\u001b[1;32m     28\u001b[0m         \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf1_micro_skai\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_save_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         dl_test = fd.DataLoader(SimpleDataset(X_test, y_test),\n",
      "\u001b[0;32m~/Research/Codeforces-clean/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/Codeforces-clean/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/Codeforces-clean/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, visualize, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/Codeforces-clean/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y, epoch)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mcopy_fp32_to_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp32_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/Codeforces-clean/fastai/model.py\u001b[0m in \u001b[0;36mtorch_item\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtorch_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mStepper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runs = 1\n",
    "out_dim = 20\n",
    "\n",
    "all_preds, all_targs = [], []\n",
    "\n",
    "for i in range(runs):\n",
    "    outer_cv = KFold(n_splits=10, shuffle=True, random_state=i+41)\n",
    "    \n",
    "    outer_cv.get_n_splits(Xall, yall)\n",
    "    for j, (nontest_i, test_i) in enumerate(outer_cv.split(Xall, yall)):\n",
    "        X_train, y_train = Xall[nontest_i], yall[nontest_i]\n",
    "        X_test, y_test = Xall[test_i], yall[test_i]\n",
    "        \n",
    "        textcnn = MWrapper(CNN_Text(in_dim, out_dim),\n",
    "                           f'{i}_cnntext20pm_{j}')\n",
    "        textcnn.model.apply(weights_init)\n",
    "\n",
    "        dl_train = fd.DataLoader(SimpleDataset(X_train, y_train),\n",
    "                                 batch_size=32, num_workers=1,\n",
    "                                 pad_idx=1, transpose=False)\n",
    "        dl_val = fd.DataLoader(SimpleDataset(X_test, y_test),\n",
    "                               batch_size=32, num_workers=1,\n",
    "                               pad_idx=1, transpose=False)\n",
    "        modeldata = fs.ModelData(str(textcnn.path), dl_train, dl_val)\n",
    "        learner = fl.Learner.from_model_data(textcnn.model,\n",
    "                                             modeldata,\n",
    "                                             opt_fn=Adam_lambda())\n",
    "        learner.metrics = [f1_micro_skai]\n",
    "        learner.fit(5e-4, 10, best_save_name='best')\n",
    "        \n",
    "        dl_test = fd.DataLoader(SimpleDataset(X_test, y_test),\n",
    "                                batch_size=32, num_workers=2,\n",
    "                                pad_idx=1, transpose=False)\n",
    "        learner.load('best')\n",
    "        preds, targs = learner.predict_dl(dl_test)\n",
    "        preds = multilabel_prediction(preds, 0.5)\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_targs.append(targs)\n",
    "        \n",
    "        print(f1_score(np.concatenate(np.array(all_targs), axis=0), \n",
    "                       np.concatenate(np.array(all_preds), axis=0), average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = np.array(all_preds)\n",
    "all_targs = np.array(all_targs)\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targs = np.concatenate(all_targs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump([all_preds, all_targs], open('data/results/cnn-ps_20m.pkl', 'wb'))\n",
    "all_preds, all_targs = pickle.load(open('data/results/cnn-ps_20m.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(all_preds[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl = hamming_loss(all_targs, all_preds)\n",
    "micro_f1 = f1_score(all_targs, all_preds, average='micro')\n",
    "macro_f1 = f1_score(all_targs, all_preds, average='macro')\n",
    "# prf_report(all_targs, all_preds, labels=m20_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss = 0.10835858585858586\n",
      "Micro_F1 = 0.33596409780253794l\n",
      "Macro_F1 = 0.2834606852644749\n"
     ]
    }
   ],
   "source": [
    "print(f'Hamming loss = {hl}\\nMicro_F1 = {micro_f1}l\\nMacro_F1 = {macro_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem-Algorithm Separate Analyis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Demo of how to separate tags into categories\n",
    "exm = all_preds[0:5]\n",
    "print(exm)\n",
    "\n",
    "prob_idxs = (2, 5, 10, 11, 12, 13, 14 , 15, 19)\n",
    "alg_idxs = (0, 1, 3, 4, 6, 7, 8, 9, 16, 17, 18)\n",
    "\n",
    "prob_targs = [exm[:, i] for i in prob_idxs]\n",
    "prob_targs = np.concatenate([prob_targs]).transpose(1, 0)\n",
    "print(prob_targs)\n",
    "\n",
    "# np.concatenate([[exm[:, 1]], [exm[:, 3]]]).transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Actual problem-algorithm splitting\n",
    "prob_idxs = (2, 5, 10, 11, 12, 13, 14 , 15, 19)\n",
    "alg_idxs = (0, 1, 3, 4, 6, 7, 8, 9, 16, 17, 18)\n",
    "\n",
    "probcat_targs = [all_targs[:, i] for i in prob_idxs]\n",
    "probcat_targs = np.concatenate([probcat_targs]).transpose(1, 0)\n",
    "print(probcat_targs)\n",
    "\n",
    "probcat_preds = [all_preds[:, i] for i in prob_idxs]\n",
    "probcat_preds = np.concatenate([probcat_preds]).transpose(1, 0)\n",
    "print(probcat_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss = 0.07867564534231201\n",
      "Micro_F1 = 0.38643326039387316l\n",
      "Macro_F1 = 0.4182498732201015\n"
     ]
    }
   ],
   "source": [
    "probcat_hl = hamming_loss(probcat_targs, probcat_preds)\n",
    "probcat_micro_f1 = f1_score(probcat_targs, probcat_preds, average='micro')\n",
    "probcat_macro_f1 = f1_score(probcat_targs, probcat_preds, average='macro')\n",
    "\n",
    "print(f'Hamming loss = {probcat_hl}\\nMicro_F1 = {probcat_micro_f1}l\\nMacro_F1 = {probcat_macro_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Actual problem-algorithm splitting\n",
    "prob_idxs = (2, 5, 10, 11, 12, 13, 14 , 15, 19)\n",
    "alg_idxs = (0, 1, 3, 4, 6, 7, 8, 9, 16, 17, 18)\n",
    "\n",
    "algcat_targs = [all_targs[:, i] for i in alg_idxs]\n",
    "algcat_targs = np.concatenate([algcat_targs]).transpose(1, 0)\n",
    "print(algcat_targs)\n",
    "\n",
    "algcat_preds = [all_preds[:, i] for i in alg_idxs]\n",
    "algcat_preds = np.concatenate([algcat_preds]).transpose(1, 0)\n",
    "print(algcat_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss = 0.13264462809917354\n",
      "Micro_F1 = 0.30835527890830744l\n",
      "Macro_F1 = 0.17317862239168946\n"
     ]
    }
   ],
   "source": [
    "algcat_hl = hamming_loss(algcat_targs, algcat_preds)\n",
    "algcat_micro_f1 = f1_score(algcat_targs, algcat_preds, average='micro')\n",
    "algcat_macro_f1 = f1_score(algcat_targs, algcat_preds, average='macro')\n",
    "\n",
    "print(f'Hamming loss = {algcat_hl}\\nMicro_F1 = {algcat_micro_f1}l\\nMacro_F1 = {algcat_macro_f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
