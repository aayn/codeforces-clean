{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Experiments - Multi-label SVM - Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayn/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# matplotlib.rcParams['figure.figsize'] = [5, 10]\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics import hamming_loss, make_scorer, confusion_matrix\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from fastai import text as ft\n",
    "from fastai import dataloader as fd\n",
    "from fastai import dataset as fs\n",
    "from fastai import learner as fl\n",
    "from fastai import core as fc\n",
    "from fastai import metrics as fm\n",
    "\n",
    "\n",
    "from skai.runner import TextRunner, Adam_lambda\n",
    "from skai.mwrapper import MWrapper, SKModel\n",
    "from skai.utils import multi_to_text_out, vote_pred\n",
    "from skai.utils import get_classification_type, weights_init, multilabel_prediction\n",
    "from skai.dataset import TokenDataset, SimpleDataset\n",
    "\n",
    "\n",
    "def mapt(f, *iters):\n",
    "    return tuple(map(f, *iters))\n",
    "\n",
    "def mapl(f, *iters):\n",
    "    return list(map(f, *iters))\n",
    "\n",
    "def manually_remove_problems(data):\n",
    "    \"\"\" remove problem from data if it has a certain tag\"\"\"\n",
    "    final_data = {}\n",
    "    remove = ['*special']\n",
    "    for i in data:\n",
    "        if set(data[i][1][0]).intersection(set(remove)) == set():\n",
    "            if data[i][0][0] != '':\n",
    "                final_data[i] = data[i]\n",
    "    return final_data\n",
    "\n",
    "def get_single_label_problems(data):\n",
    "    '''returns a dict of all problems which only have one label'''\n",
    "    single_label_problems = {}\n",
    "    for i in data:\n",
    "        if len(data[i][1][0]) == 1:\n",
    "            single_label_problems[i] = data[i]\n",
    "    return single_label_problems\n",
    "\n",
    "def get_classwise_distribution(data):\n",
    "    class_count = {}\n",
    "    for i in data:\n",
    "        for cls in data[i][1][0]:\n",
    "            if cls in class_count:\n",
    "                class_count[cls] +=1 \n",
    "            else:\n",
    "                class_count[cls] = 1\n",
    "    return class_count\n",
    "\n",
    "\n",
    "def get_topk_single_label_problems(data,k):\n",
    "    \"\"\" get top k by frequency single label problems\"\"\"\n",
    "    class_dict = get_classwise_distribution(data)\n",
    "    print(class_dict)\n",
    "    class_dict = dict(sorted(class_dict.items(), key=itemgetter(1), reverse=True)[:k])\n",
    "    print(set(class_dict.keys()))\n",
    "\n",
    "    topk_data = {}\n",
    "    for i in data:\n",
    "        if set(data[i][1][0]).intersection(set(class_dict.keys())) != set():\n",
    "            topk_data[i] = data[i]\n",
    "            \n",
    "    return topk_data\n",
    "\n",
    "def make_text_dataset(rdata):\n",
    "    Xtext, ytext = [], []\n",
    "    for url, data in rdata.items():\n",
    "        try:\n",
    "            ytext.append(data[1][0][0])\n",
    "        except IndexError:\n",
    "            continue\n",
    "        Xtext.append(data[0][0])\n",
    "    return Xtext, ytext\n",
    "\n",
    "def make_multi_text_dataset(rdata):\n",
    "    Xtext, ytext = [], []\n",
    "    for url, data in rdata.items():\n",
    "        try:\n",
    "            ytext.append(data[1][0])\n",
    "        except IndexError:\n",
    "            continue\n",
    "        Xtext.append(data[0][0])\n",
    "    return Xtext, ytext\n",
    "\n",
    "def make_statement_dataset(rdata):\n",
    "    Xtext, ytext = [], []\n",
    "    for url, data in rdata.items():\n",
    "        try:\n",
    "            ytext.append(data[1][0])\n",
    "        except IndexError:\n",
    "            continue\n",
    "        Xtext.append(data[0][2])\n",
    "    return Xtext, ytext\n",
    "\n",
    "def make_non_statement_dataset(rdata):\n",
    "    Xtext, ytext = [], []\n",
    "    for url, data in rdata.items():\n",
    "        try:\n",
    "            ytext.append(data[1][0][0])\n",
    "        except IndexError:\n",
    "            continue\n",
    "        Xtext.append(f'{data[0][3]}\\n{data[0][4]}\\n{data[0][5]}')\n",
    "    return Xtext, ytext\n",
    "\n",
    "def get_class_list(labels):\n",
    "    return list(set(labels))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(22, 16)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=1.0)\n",
    "#     plt.title(title, fontsize)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=32)\n",
    "    plt.yticks(tick_marks, classes, fontsize=32)\n",
    "\n",
    "    print(cm.max())\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = 0.5\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                 fontsize=32)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=32)\n",
    "    plt.xlabel('Predicted label', fontsize=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10m = pickle.load(open('data/10multi_26aug.pkl','rb'))\n",
    "top20m = pickle.load(open('data/20multi_26aug.pkl','rb'))\n",
    "\n",
    "top10pm, top20pm = mapt(make_statement_dataset, [top10m, top20m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You've got array a[1], a[2], ..., a[n], consisting of n integers. Count the number of ways to split all the elements of the array into three contiguous parts so that the sum of elements in each part is the same.\n",
      "More formally, you need to find the number of such pairs of indices i, j (2 ≤ i ≤ j ≤ n - 1), that .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(top10pm[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary search', 'implementation', 'data structures']\n"
     ]
    }
   ],
   "source": [
    "print(top20pm[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Model directory for svm_cv exists.\n"
     ]
    }
   ],
   "source": [
    "svm_cv = SKModel(Pipeline(\n",
    "    [('countvec', CountVectorizer(stop_words=stop_words)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))]),\n",
    "    {'countvec__max_df': (0.25, 0.5, 0.75),\n",
    "    'countvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "    \"clf__estimator__class_weight\": ['balanced', None]})\n",
    "svm_cv = MWrapper(svm_cv, 'svm_cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('countvec',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 3), preprocessor=None,\n",
       "           stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('clf',\n",
       "   OneVsRestClassifier(estimator=LinearSVC(C=0.01, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "        intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "        multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "        verbose=0),\n",
       "             n_jobs=1))],\n",
       " 'countvec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 3), preprocessor=None,\n",
       "         stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'clf': OneVsRestClassifier(estimator=LinearSVC(C=0.01, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "      verbose=0),\n",
       "           n_jobs=1),\n",
       " 'countvec__analyzer': 'word',\n",
       " 'countvec__binary': False,\n",
       " 'countvec__decode_error': 'strict',\n",
       " 'countvec__dtype': numpy.int64,\n",
       " 'countvec__encoding': 'utf-8',\n",
       " 'countvec__input': 'content',\n",
       " 'countvec__lowercase': True,\n",
       " 'countvec__max_df': 0.5,\n",
       " 'countvec__max_features': None,\n",
       " 'countvec__min_df': 1,\n",
       " 'countvec__ngram_range': (1, 3),\n",
       " 'countvec__preprocessor': None,\n",
       " 'countvec__stop_words': ['i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  \"you're\",\n",
       "  \"you've\",\n",
       "  \"you'll\",\n",
       "  \"you'd\",\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  \"she's\",\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  \"it's\",\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  \"that'll\",\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  \"don't\",\n",
       "  'should',\n",
       "  \"should've\",\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  \"aren't\",\n",
       "  'couldn',\n",
       "  \"couldn't\",\n",
       "  'didn',\n",
       "  \"didn't\",\n",
       "  'doesn',\n",
       "  \"doesn't\",\n",
       "  'hadn',\n",
       "  \"hadn't\",\n",
       "  'hasn',\n",
       "  \"hasn't\",\n",
       "  'haven',\n",
       "  \"haven't\",\n",
       "  'isn',\n",
       "  \"isn't\",\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  \"mightn't\",\n",
       "  'mustn',\n",
       "  \"mustn't\",\n",
       "  'needn',\n",
       "  \"needn't\",\n",
       "  'shan',\n",
       "  \"shan't\",\n",
       "  'shouldn',\n",
       "  \"shouldn't\",\n",
       "  'wasn',\n",
       "  \"wasn't\",\n",
       "  'weren',\n",
       "  \"weren't\",\n",
       "  'won',\n",
       "  \"won't\",\n",
       "  'wouldn',\n",
       "  \"wouldn't\"],\n",
       " 'countvec__strip_accents': None,\n",
       " 'countvec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'countvec__tokenizer': None,\n",
       " 'countvec__vocabulary': None,\n",
       " 'clf__estimator__C': 0.01,\n",
       " 'clf__estimator__class_weight': 'balanced',\n",
       " 'clf__estimator__dual': True,\n",
       " 'clf__estimator__fit_intercept': True,\n",
       " 'clf__estimator__intercept_scaling': 1,\n",
       " 'clf__estimator__loss': 'squared_hinge',\n",
       " 'clf__estimator__max_iter': 1000,\n",
       " 'clf__estimator__multi_class': 'ovr',\n",
       " 'clf__estimator__penalty': 'l2',\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__tol': 0.0001,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator': LinearSVC(C=0.01, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "      verbose=0),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('countvec',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
       "           strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None)),\n",
       "  ('clf',\n",
       "   OneVsRestClassifier(estimator=LinearSVC(C=1, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "        intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "        multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "        verbose=0),\n",
       "             n_jobs=1))],\n",
       " 'countvec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
       "         strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       " 'clf': OneVsRestClassifier(estimator=LinearSVC(C=1, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "      verbose=0),\n",
       "           n_jobs=1),\n",
       " 'countvec__analyzer': 'word',\n",
       " 'countvec__binary': False,\n",
       " 'countvec__decode_error': 'strict',\n",
       " 'countvec__dtype': numpy.int64,\n",
       " 'countvec__encoding': 'utf-8',\n",
       " 'countvec__input': 'content',\n",
       " 'countvec__lowercase': True,\n",
       " 'countvec__max_df': 0.5,\n",
       " 'countvec__max_features': None,\n",
       " 'countvec__min_df': 1,\n",
       " 'countvec__ngram_range': (1, 3),\n",
       " 'countvec__norm': 'l2',\n",
       " 'countvec__preprocessor': None,\n",
       " 'countvec__smooth_idf': True,\n",
       " 'countvec__stop_words': ['i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  \"you're\",\n",
       "  \"you've\",\n",
       "  \"you'll\",\n",
       "  \"you'd\",\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  \"she's\",\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  \"it's\",\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  \"that'll\",\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  \"don't\",\n",
       "  'should',\n",
       "  \"should've\",\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  \"aren't\",\n",
       "  'couldn',\n",
       "  \"couldn't\",\n",
       "  'didn',\n",
       "  \"didn't\",\n",
       "  'doesn',\n",
       "  \"doesn't\",\n",
       "  'hadn',\n",
       "  \"hadn't\",\n",
       "  'hasn',\n",
       "  \"hasn't\",\n",
       "  'haven',\n",
       "  \"haven't\",\n",
       "  'isn',\n",
       "  \"isn't\",\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  \"mightn't\",\n",
       "  'mustn',\n",
       "  \"mustn't\",\n",
       "  'needn',\n",
       "  \"needn't\",\n",
       "  'shan',\n",
       "  \"shan't\",\n",
       "  'shouldn',\n",
       "  \"shouldn't\",\n",
       "  'wasn',\n",
       "  \"wasn't\",\n",
       "  'weren',\n",
       "  \"weren't\",\n",
       "  'won',\n",
       "  \"won't\",\n",
       "  'wouldn',\n",
       "  \"wouldn't\"],\n",
       " 'countvec__strip_accents': None,\n",
       " 'countvec__sublinear_tf': False,\n",
       " 'countvec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'countvec__tokenizer': None,\n",
       " 'countvec__use_idf': True,\n",
       " 'countvec__vocabulary': None,\n",
       " 'clf__estimator__C': 1,\n",
       " 'clf__estimator__class_weight': 'balanced',\n",
       " 'clf__estimator__dual': True,\n",
       " 'clf__estimator__fit_intercept': True,\n",
       " 'clf__estimator__intercept_scaling': 1,\n",
       " 'clf__estimator__loss': 'squared_hinge',\n",
       " 'clf__estimator__max_iter': 1000,\n",
       " 'clf__estimator__multi_class': 'ovr',\n",
       " 'clf__estimator__penalty': 'l2',\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__tol': 0.0001,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator': LinearSVC(C=1, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "      verbose=0),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_clf.get_params()\n",
    "\n",
    "# {'countvec__max_df': (0.25, 0.5, 0.75),\n",
    "#     'countvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "#     \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "#     \"clf__estimator__class_weight\": ['balanced', None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Model directory for svm_cv exists.\n",
      "Note: Checkpoints directory for svm_cv exists.\n"
     ]
    }
   ],
   "source": [
    "svm_cv = SKModel(Pipeline(\n",
    "    [('countvec', CountVectorizer(stop_words=stop_words, max_df=0.5)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))]),\n",
    "    {'countvec__ngram_range': [(1, 3)],\n",
    "    \"clf__estimator__C\": [0.01],\n",
    "    \"clf__estimator__class_weight\": ['balanced']})\n",
    "svm_cv = MWrapper(svm_cv, 'svm_cv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20-class experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint reached: raw data cleaned.\n",
      "multilabel classification.\n"
     ]
    }
   ],
   "source": [
    "trunner = TextRunner([svm_cv], top20pm[0], top20pm[1], 'top20pm', make_pyt_data=False)\n",
    "Xall, yall = np.array(trunner.rdata), np.array(trunner.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer split no. 0\n",
      "{'number theory', 'geometry', 'bitmasks', 'implementation', 'two pointers', 'greedy', 'binary search', 'math', 'dsu', 'brute force', 'dfs and similar', 'constructive algorithms', 'sortings', 'data structures', 'graphs', 'combinatorics', 'dp', 'probabilities', 'trees', 'strings'}\n",
      "20\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    7.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09090909090909091\n",
      "Outer split no. 1\n",
      "{'number theory', 'geometry', 'bitmasks', 'implementation', 'two pointers', 'greedy', 'binary search', 'math', 'dsu', 'brute force', 'dfs and similar', 'constructive algorithms', 'sortings', 'data structures', 'graphs', 'combinatorics', 'dp', 'probabilities', 'trees', 'strings'}\n",
      "20\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    8.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07954545454545454\n",
      "Outer split no. 2\n",
      "{'number theory', 'geometry', 'bitmasks', 'implementation', 'two pointers', 'greedy', 'binary search', 'math', 'dsu', 'brute force', 'dfs and similar', 'sortings', 'combinatorics', 'data structures', 'graphs', 'constructive algorithms', 'dp', 'probabilities', 'trees', 'strings'}\n",
      "20\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    7.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07744107744107744\n",
      "Outer split no. 3\n",
      "{'number theory', 'geometry', 'bitmasks', 'implementation', 'two pointers', 'greedy', 'binary search', 'math', 'dsu', 'brute force', 'dfs and similar', 'constructive algorithms', 'sortings', 'data structures', 'graphs', 'combinatorics', 'dp', 'probabilities', 'trees', 'strings'}\n",
      "20\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    8.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07765151515151515\n",
      "Outer split no. 4\n",
      "{'number theory', 'geometry', 'bitmasks', 'implementation', 'two pointers', 'greedy', 'binary search', 'math', 'dsu', 'brute force', 'dfs and similar', 'constructive algorithms', 'sortings', 'data structures', 'graphs', 'combinatorics', 'dp', 'probabilities', 'trees', 'strings'}\n",
      "20\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    7.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07727272727272727\n",
      "Outer split no. 5\n",
      "{'number theory', 'geometry', 'greedy', 'implementation', 'two pointers', 'bitmasks', 'binary search', 'math', 'dsu', 'brute force', 'dfs and similar', 'constructive algorithms', 'sortings', 'data structures', 'graphs', 'combinatorics', 'dp', 'probabilities', 'trees', 'strings'}\n",
      "20\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    7.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0787037037037037\n",
      "Outer split no. 6\n",
      "{'number theory', 'geometry', 'bitmasks', 'implementation', 'two pointers', 'greedy', 'binary search', 'math', 'dsu', 'brute force', 'dfs and similar', 'constructive algorithms', 'combinatorics', 'data structures', 'graphs', 'sortings', 'dp', 'probabilities', 'trees', 'strings'}\n",
      "20\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    8.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07611832611832611\n",
      "Outer split no. 7\n",
      "{'number theory', 'geometry', 'bitmasks', 'implementation', 'two pointers', 'greedy', 'binary search', 'math', 'constructive algorithms', 'brute force', 'dfs and similar', 'sortings', 'dsu', 'data structures', 'combinatorics', 'graphs', 'dp', 'probabilities', 'trees', 'strings'}\n",
      "20\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    7.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07512626262626262\n",
      "Outer split no. 8\n",
      "{'number theory', 'geometry', 'bitmasks', 'implementation', 'two pointers', 'greedy', 'binary search', 'math', 'dsu', 'brute force', 'dfs and similar', 'constructive algorithms', 'sortings', 'data structures', 'graphs', 'combinatorics', 'dp', 'probabilities', 'trees', 'strings'}\n",
      "20\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07463524130190796\n",
      "Outer split no. 9\n",
      "{'number theory', 'geometry', 'bitmasks', 'implementation', 'greedy', 'two pointers', 'binary search', 'math', 'dsu', 'dfs and similar', 'constructive algorithms', 'brute force', 'sortings', 'data structures', 'graphs', 'combinatorics', 'dp', 'probabilities', 'trees', 'strings'}\n",
      "20\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0755050505050505\n"
     ]
    }
   ],
   "source": [
    "runs = 1\n",
    "out_dim = 20\n",
    "\n",
    "preds_txt_cv, preds_txt_tf = [], []\n",
    "targs_txt = []\n",
    "\n",
    "for i in range(runs):\n",
    "    outer_cv = KFold(n_splits=10, shuffle=True, random_state=i+42)\n",
    "    \n",
    "    outer_cv.get_n_splits(Xall, yall)\n",
    "    for j, (nontest_i, test_i) in enumerate(outer_cv.split(Xall, yall)):\n",
    "        print(f'Outer split no. {j}')\n",
    "        X_train, y_train = Xall[nontest_i], yall[nontest_i]\n",
    "        X_test, y_test = Xall[test_i], yall[test_i]\n",
    "        \n",
    "        cv_clf, cv_score = trunner.get_clf_sk(svm_cv, X_train, y_train)\n",
    "        \n",
    "        preds = cv_clf.predict(X_test)\n",
    "        preds_txt_cv.append(preds)\n",
    "        \n",
    "        y_test = trunner.alldata.ovectorizer.transform(y_test)\n",
    "        targs_txt.append(y_test)\n",
    "\n",
    "        print(accuracy_score(np.concatenate(targs_txt),\n",
    "                             np.concatenate(preds_txt_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_txt_cv = np.concatenate(preds_txt_cv)\n",
    "targs_txt = np.concatenate(targs_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump([preds_txt_cv, targs_txt], open('data/results/svm-ps_20m.pkl', 'wb'))\n",
    "preds_txt_cv, targs_txt = pickle.load(open('data/results/svm-ps_20m.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss = 0.10957070707070707\n",
      "Micro_F1 = 0.2974417098445596l\n",
      "Macro_F1 = 0.2556668833871532\n"
     ]
    }
   ],
   "source": [
    "hl = hamming_loss(targs_txt, preds_txt_cv)\n",
    "micro_f1 = f1_score(targs_txt, preds_txt_cv, average='micro')\n",
    "macro_f1 = f1_score(targs_txt, preds_txt_cv, average='macro')\n",
    "\n",
    "print(f'Hamming loss = {hl}\\nMicro_F1 = {micro_f1}l\\nMacro_F1 = {macro_f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
